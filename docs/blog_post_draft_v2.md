# The Architecture of Impossibility: Building a 3D Game Engine in SQL

"SQL is for data analysis."

I must have heard that phrase—or said it myself—a thousand times. We use SQL to query tables, aggregate numbers, and maybe, if we're feeling adventurous, generate some complex reports. It's the reliable, boring workhorse of the backend.

But recently, I’ve been thinking a lot about what happens when you push a technology so far past its breaking point that it becomes something else entirely.

What if you used a database not to store game data, but to *be* the game engine?

I’m not talking about storing high scores. I’m talking about real-time, 3D raycasting, texture mapping, and lighting—all calculated inside a `SELECT` statement.

Welcome to **DOOMHouse**.

## The Agentic Workflow

I won't lie—I didn't write this code. At least, not in the traditional sense.

If I had tried to hand-write the vector math required to raycast a 3D world inside a ClickHouse SQL query, I probably would have given up after the first syntax error. The complexity is just... unreasonable.

Instead, this project was constructed through what I’m calling an **Agentic Workflow**. It wasn't just "asking ChatGPT to write code." It was a coordinated effort using multiple specialized AI models, treating them like a team of senior engineers.

Here was the lineup:
*   **Gemini 3.0 Pro & Flash**: The heavy lifters. They handled the core logic and the brute-force generation of the initial SQL queries.
*   **Claude 3.5 Opus**: The architect. When the logic got tangled, I turned to Opus for high-level reasoning and architectural decisions.
*   **ChatGPT 5.2**: The artist. Specifically tasked with generating the texture maps and handling the visual data structures.

It felt less like coding and more like conducting an orchestra. An orchestra of silicon brains that occasionally hallucinated, but often produced brilliance.

## Iterative Optimization: "Optimize the Optimized"

The real magic happened in the optimization loop.

The first working version was... slow. Painfully slow. A human developer might have looked at the query plan, tweaked an index, and called it a day. But with the agents, I could do something different.

I fed the code back into the models with a simple, recursive instruction: *"Optimize this."*

Then I took the result and said, *"Optimize it again."*

And again.

We weren't just refactoring; we were squeezing every ounce of performance out of the syntax. The agents found vectorization tricks and mathematical shortcuts that I’m not sure I would have ever spotted. They turned loops into array operations and conditional logic into bitwise arithmetic. It was iterative refinement on steroids.

## Visual Debugging

Debugging SQL is usually about looking at rows and columns. But how do you debug a 3D render when the "bug" is a perspective error or a texture artifact?

You can't just look at the logs.

So, we created a visual feedback loop. When the engine rendered a frame with a glitch—say, a wall that looked like it was melting or a floor that vanished—I took a screenshot.

I fed that image back to the agents. *"Look at this. The wall on the left is distorted. Why?"*

The models analyzed the visual output, correlated it with the math in the SQL query, and proposed fixes. It was a surreal experience: debugging code by showing the computer a picture of its own mistake.

## The Architecture of Impossibility

The result of this madness is what I call the **Architecture of Impossibility**.

To make this run in real-time, we couldn't just run one big query. The computational load was too immense. The solution—proposed and refined by the agents—was to split the workload into **four parallel pipelines**.

1.  **Raycasting**: Calculating the geometry.
2.  **Texture Mapping**: Looking up pixel data.
3.  **Shading/Lighting**: Applying distance-based fog and contrast.
4.  **Post-Processing**: A SWAR (SIMD Within A Register) based blur filter for smoothing.

All of this happens *inside* the database. The Python client is just a dumb terminal; it sends input and displays the resulting array of pixels. The database is the GPU.

## Breaking the Database

We pushed so hard that we actually broke things.

At one point, the SQL generated by the agents was so complex—nested arrays, high-order functions, bitwise operations—that it exposed a genuine bug in ClickHouse itself.

The query caused a crash that shouldn't have been possible. We ended up reporting it, and it helped the ClickHouse team verify a solution for their engine.

There’s something satisfying about that. We weren't just using the tool; we were stress-testing it to its absolute limit.

## The Unknown

It’s not perfect.

Looking back, there are things I wish we had tracked better. I don't have detailed analytics on the token usage or the logs of the models' "thinking" processes during the build. That metadata is lost to the ether.

And if you look closely at the code, there are parts that are... alien. Logic structures that work perfectly but are incredibly difficult for a human to parse. It’s a "black box" in the truest sense.

## The Future of Engineering

This project started as a "what if." But it left me with a profound realization about the future of software engineering.

We are entering an era where AI agents can generate code of such complexity that it exceeds our ability to write—and perhaps even fully understand—manually.

Does that mean the human developer is obsolete?

No. But our role is changing. We are becoming architects, orchestrators, and debuggers of systems that are grown rather than built. We define the constraints, we guide the evolution, and we verify the results.

DOOMHouse is a toy. But the process used to build it? That’s the future.

And honestly? It’s going to be a wild ride.
